<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Projects and Resources | Laura Cabrera-Quiros</title>
<meta name="generator" content="Jekyll v4.1.0" />
<meta property="og:title" content="Projects and Resources" />
<meta name="author" content="Dra.-Ing. Laura Cabrera-Quiros" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Datasets" />
<meta property="og:description" content="Datasets" />
<link rel="canonical" href="http://localhost:4000/Projects/" />
<meta property="og:url" content="http://localhost:4000/Projects/" />
<meta property="og:site_name" content="Laura Cabrera-Quiros" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/Projects/","author":{"@type":"Person","name":"Dra.-Ing. Laura Cabrera-Quiros"},"headline":"Projects and Resources","description":"Datasets","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Laura Cabrera-Quiros" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Laura Cabrera-Quiros</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/news/">News</a><a class="page-link" href="/Projects/">Projects and Resources</a><a class="page-link" href="/about/">About me</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Projects and Resources</h1>
  </header>

  <div class="post-content">
    <h2 id="datasets">Datasets</h2>

<h3 id="match-n-mingle-dataset">Match N Mingle dataset</h3>

<p><img style="float: left;" src="../img/modalities.svg" width="500" /></p>

<p>The MatchNMingle (MnM) is a novel multi-sensor resource for the analysis of social interactions and group dynamics in-the-wild during free-standing conversations and speed dates, available openly for research purposes. It was recorded in-the-wild during 3 speed date events, each followed by a mingle (cocktail party). All events took place in a public bar with a total of 92 participants which were not acquaintance before the event.</p>

<p>It consists of 2 hours of data from wearable acceleration, binary proximity, video, audio, personality surveys, frontal pictures and speed-date responses. Participantsâ€™ positions and group formations were manually annotated; as were social actions (walking, stepping, drinking, speaking, hand gesture, head gesture, laugh) for 30 minutes at 20fps making it the first dataset to incorporate the annotation of such cues in this context</p>

<p>For more information, please visit the dataset <a href="http://matchmakers.ewi.tudelft.nl/matchnmingle/pmwiki/">website</a>.</p>

<h2 id="current-projects">Current projects</h2>

<p>TBA</p>

<h2 id="previous-projects">Previous projects</h2>

<h3 id="alarm-project">ALARM project</h3>

<p><img style="float: right;" src="../img/NICU.jpg" width="250" /></p>

<p>The ALARM (Alarm-Limiting AlgoRithm-based Monitoring) project consisted of model-based predictive monitoring for neonatal intensive care units, developed by the <a href="https://www.tue.nl/en/research/research-groups/eindhoven-medtech-innovation-center/">e/MTIC</a> innitiative and in a collaboration between TU Eindhoven, the <a href="https://www.mmc.nl/">Maxima Medical Center</a> (MMC) and <a href="https://www.philips.com/a-w/research/locations/eindhoven.html">Philips research</a> (Eindhoven headquarters).</p>

<p>The project aims to develop new monitoring techniques for improved and efficient diagnosis, but also leveragin data mining approaches to reduce the high number of alarms in the intensive care environment (neonatal and adult ICU) in order to increase patient safety and reduce alarm fatigue of the staff.  Neonates infants are a particualrly vulnerable population, and improving monitoring of maturation of neonates is important to predict patient outcome.</p>

<p>The project is lead by Dr. Carola van Pul (<a href="https://www.tue.nl/en/research/researchers/carola-van-pul/">contact</a>).</p>

<h3 id="multimodal-detection-of-gestures-in-the-wild">Multimodal detection of gestures in-the-wild</h3>

<p><img style="float: left;" src="../img/gestures.jpg" width="250" /></p>

<p>Whereas most efforts related to gestures are focus on applications for Human-Computer interaction, not much has been done about detecting gestures as an inherent part of social interactions.</p>

<p>This project addresses the detection of hand gestures during free-standing conversations in crowded mingle scenarios. Unlike the scenarios of the previous works in gesture detection and recognition, crowded mingle scenes have additional challenges such as cross-contamination between subjects, strong occlusions, and nonstationary backgrounds. This makes them more complex to analyze using computer vision techniques alone.</p>

<p>We propose a multimodal approach using video and wearable acceleration data recorded via smart badges hung around the neck. In the video modality, we propose to treat noisy dense trajectories as bags-of-trajectories. For a given bag, we can have good trajectories corresponding to the subject, and bad trajectories due for instance to cross-contamination. However, we hypothesize that for a given class, it should be possible to learn trajectories that are discriminative while ignoring noisy trajectories. We do this by exploiting multiple instance learning via embedded instance selection as our multiple instance learning approach.</p>

<p>See related paper <a href="https://ieeexplore.ieee.org/abstract/document/8734888/">here</a>.</p>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Dra.-Ing. Laura Cabrera-Quiros</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Assistant Professor<br>Electronics Engineering<br>TEC Costa Rica<!---Dra.-Ing. Laura Cabrera-Quiros--->
            </li><li><a class="u-email" href="mailto:lcabrera@itcr.ac.cr">lcabrera@itcr.ac.cr</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
	      <!--- <p></p> 
	      <img style="float: right;" src="../img/TEC_logo.png" width="100"/>--->
      </div>
    </div>

  </div>

</footer>
</body>

</html>
